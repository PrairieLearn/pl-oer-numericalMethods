{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "import scipy.linalg as sla\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from matplotlib import animation,rc\n",
        "from IPython.display import HTML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# Finite Difference Method "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 1) $f: R \\rightarrow R$\n",
        "\n",
        "\n",
        "Suppose you have the function:\n",
        "\n",
        "$$ f(x) = e^x - 2 $$\n",
        "\n",
        "and we want to use FD difference method to compute $\\frac{df}{dx}$. We will first define the functions to evaluate $f(x)$ and $\\frac{df}{dx}(x)$ exactly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    return np.exp(x) - 2\n",
        "\n",
        "def df(x):\n",
        "    return np.exp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-1, 2, 100)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, f(x), lw=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "Let's evaluate the first derivative using finite difference approximation for decreasing values of h. In this example, we want to approximate the derivative at $x = \\hat{x} = 1.0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        }
      },
      "outputs": [],
      "source": [
        "xhat = 1.0  # point where we want to find the approximation\n",
        "h = 1.0     # initial perturbation\n",
        "errors = []\n",
        "hs = []\n",
        "\n",
        "fval = f(xhat)  # we only need to evaluate this once!\n",
        "\n",
        "# in general, we don't have this value, but we will use it here to visualize the error\n",
        "dfexact = df(xhat)  \n",
        "\n",
        "for i in range(20):\n",
        "    \n",
        "    # one function evaluation per each perturbation size\n",
        "    dfapprox = ( f(xhat+h) - fval ) / h   \n",
        "    \n",
        "    # get the error\n",
        "    err = np.abs(dfexact - dfapprox)\n",
        "    \n",
        "    print(\" %E \\t %E \" %(h, err) )\n",
        "    \n",
        "    hs.append(h)\n",
        "    errors.append(err)\n",
        "    \n",
        "    h = h / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.loglog(hs, errors, lw=3)\n",
        "plt.xlabel('h')\n",
        "plt.ylabel('error')\n",
        "plt.xlim([1e-6,1])\n",
        "plt.ylim([1e-6,1])\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "What happens if we keep decreasing the perturbation h?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "xhat = 1.0  # point where we want to find the approximation\n",
        "h = 1.0     # initial perturbation\n",
        "errors = []\n",
        "hs = []\n",
        "\n",
        "fval = f(xhat)  # we only need to evaluate this once!\n",
        "\n",
        "# in general, we don't have this value, but we will use it here to visualize the error\n",
        "dfexact = df(xhat)  \n",
        "\n",
        "for i in range(80):\n",
        "    \n",
        "    # one function evaluation per each perturbation size\n",
        "    dfapprox = ( f(xhat+h) - fval ) / h   \n",
        "    \n",
        "    # get the error\n",
        "    err = np.abs(dfexact - dfapprox)\n",
        "    \n",
        "    print(\" %E \\t %E \" %(h, err) )\n",
        "    \n",
        "    hs.append(h)\n",
        "    errors.append(err)\n",
        "    \n",
        "    h = h / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.loglog(hs, errors, lw=3)\n",
        "plt.xlabel('h')\n",
        "plt.ylabel('error')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "Not what we expected, correct? If we plot the asymptotic behavior of the truncation error, the error continues to become smaller as the perturbation size decreases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.loglog(hs, errors, lw=3,label='total')\n",
        "plt.loglog(hs,np.array(hs),'--',label='truncation')\n",
        "plt.loglog(hs,1e-16/np.array(hs),'--',label='rounding')\n",
        "plt.legend(bbox_to_anchor=(1.1, 1.05))\n",
        "plt.xlabel('h')\n",
        "plt.ylabel('error')\n",
        "plt.grid()\n",
        "plt.ylim(1e-16,1e2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "-"
        }
      },
      "source": [
        "However, as the perturbation decreases, cancelation starts to take effect when computing the finite difference. There is a competition between the truncation error and the rounding error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## 2) $f: R^n \\rightarrow R$\n",
        "\n",
        "Suppose you have the function:\n",
        "\n",
        "$$ f(x_1, x_2) = 2 x_1 + x_1^2 \\, x_2 + x_2^3 $$\n",
        "\n",
        "and we want to use FD difference method to compute $\\frac{\\partial f}{\\partial x_1}$ and $\\frac{\\partial f}{\\partial x_2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(x1,x2):\n",
        "    return (2*x1 + (x1**2)*x2 + x2**3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will need to add a perturbation to each one of the function variables $x_i$. The \"ideal\" perturbation is not necessarily the same for each variable, but it can be a good starting point. We will perform these computations for $x_1 = 1.3$ and $x_2 = 4.9$, using the Forward Finite Difference Method. The exact solution is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1 = 1.3\n",
        "x2 = 4.9\n",
        "dfdx1_exact = 2 + 2*x1*x2\n",
        "dfdx2_exact = x1**2 + 3*x2**2\n",
        "print(dfdx1_exact,dfdx2_exact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h = 0.05\n",
        "fval = f(x1,x2)\n",
        "\n",
        "dfx1 = (f(x1+h,x2)-fval)/h\n",
        "dfx2 = (f(x1,x2+h)-fval)/h\n",
        "\n",
        "print(dfx1, dfdx1_exact, abs((dfx1-dfdx1_exact)/dfdx1_exact) )\n",
        "print(dfx2, dfdx2_exact, abs((dfx2-dfdx2_exact)/dfdx2_exact) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that in this case, you have as many derivatives as you have variables. Indeed, your function derivative follows:\n",
        "\n",
        "\n",
        "$$f': R^n \\rightarrow R^n $$\n",
        "\n",
        "You can think of storing these quantities as an array. Supposed the function is:\n",
        "\n",
        "$$ f({\\bf x}) = f(x_1, x_2, x_3) = 2 x_1 x_3 + x_1^2 \\, x_2 + x_2^3 + 6 x_3^2$$\n",
        "\n",
        "But we will define it as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(xvec):\n",
        "    return (2*xvec[0]*xvec[2] + (xvec[0]**2)*xvec[1] + xvec[1]**3 + 6*xvec[2]**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In general, for a function $f(\\bf x)$, the derivative can be expressed as:\n",
        "\n",
        "    \n",
        "$$ \\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1}\\\\\\frac{\\partial f}{\\partial x_2}\\\\...\\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} $$\n",
        "\n",
        "which is called the **gradient** of $f$. The exact array can be obtained using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradf(xvec):\n",
        "    x1,x2,x3 = xvec\n",
        "    df1 = 2*x3 + 2*x1*x2\n",
        "    df2 = x1**2 + 3*x2**2\n",
        "    df3 = 2*x1 + 12*x3\n",
        "    return np.array([df1,df2,df3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will perform our computation using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xvec = np.array([4.1, 3.3, 15.8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember that we need to add the perturbation one entry at a time! We should also compute the function value for the unperturbed variables only once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fval = f(xvec)\n",
        "print(fval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h = 0.1\n",
        "df_grad = np.zeros_like(xvec)\n",
        "for i in range(len(xvec)):\n",
        "    xfd = xvec.copy()\n",
        "    xfd[i] += h\n",
        "    df_grad[i] = (f(xfd) - fval)/h\n",
        "\n",
        "print(df_grad)\n",
        "print(gradf(xvec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) $f: R^n \\rightarrow R^m$\n",
        "\n",
        "Suppose you have the function ${\\bf f}({\\bf x})$ where ${\\bf x} \\in R^n$:\n",
        "\n",
        "$$ {\\bf f}({\\bf x}) =  \\begin{bmatrix} f_1({\\bf x})\\\\f_2({\\bf x})\\\\ ... \\\\f_m({\\bf x}) \\end{bmatrix} $$\n",
        "\n",
        "The derivative of this function is:\n",
        "\n",
        "$$ {\\bf J}({\\bf x}) =  \\begin{bmatrix} \n",
        "\\frac{\\partial f_1}{\\partial x_1}({\\bf x}) & ... & \\frac{\\partial f_1}{\\partial x_i}({\\bf x}) & ... & \\frac{\\partial f_1}{\\partial x_n}({\\bf x})\\\\\n",
        " & & \\ddots & &\\\\\n",
        "\\frac{\\partial f_m}{\\partial x_1}({\\bf x}) & ... & \\frac{\\partial f_m}{\\partial x_i}({\\bf x}) & ... & \\frac{\\partial f_m}{\\partial x_n}({\\bf x})\\\\\n",
        "\\end{bmatrix}  =\n",
        "\\begin{bmatrix} \n",
        "\\frac{\\partial {\\bf f}}{\\partial x_1} & ... & \\frac{\\partial {\\bf f}}{\\partial x_i} & ... & \\frac{\\partial {\\bf f}}{\\partial x_n}\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "which is known as the **Jacobian** matrix.\n",
        "\n",
        "Here is another example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fn(xvec):\n",
        "    f1 = 2*xvec[0]*xvec[2] + (xvec[0]**2)*xvec[1] + xvec[1]**3 + 6*xvec[2]**2\n",
        "    f2 = xvec[0]*xvec[1]*xvec[2]   \n",
        "    return np.array([f1,f2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xvec = np.array([42.1, 33.3, 15.8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fn(xvec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The exact Jacobian is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Jacobianf(xvec):\n",
        "    x1,x2,x3 = xvec\n",
        "    df1dx1 = 2*x3 + 2*x1*x2\n",
        "    df1dx2 = x1**2 + 3*x2**2\n",
        "    df1dx3 = 2*x1 + 12*x3\n",
        "    df2dx1 = x2*x3\n",
        "    df2dx2 = x1*x3\n",
        "    df2dx3 = x2*x1\n",
        "    return np.array([[df1dx1,df1dx2,df1dx3],[df2dx1,df2dx2,df2dx3]])   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Jacobianf(xvec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using finite difference method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h = 0.001\n",
        "m = 2\n",
        "n = 3\n",
        "Jac = np.zeros((m,n))\n",
        "fvaln = fn(xvec)\n",
        "\n",
        "for i in range(n):\n",
        "    xfd = xvec.copy()\n",
        "    xfd[i] += h\n",
        "    Jac[:,i] = (fn(xfd) - fvaln)/h\n",
        "\n",
        "print(Jac)\n",
        "print(Jacobianf(xvec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diffusion Equation Finite Difference equation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\">\n",
        "\n",
        "<h1>Motivation: Cooling of a rod given an initial temperature </h1>\n",
        "\n",
        "The time-dependent diffusion equation is a partial differential equation (PDE) that is often used to model heat transfer problems:\n",
        "    \n",
        "$$\\frac{\\partial u}{\\partial t} - D\\frac{\\partial^2 u}{\\partial x^2} = 0$$\n",
        "    \n",
        "where $u$ is the temperature that varies in space $x$ and time $t$, i.e., $u(x,t)$. $D$ is the thermal diffusivity.\n",
        "    \n",
        "Boundary conditions: $u(-1,t) = 0 \\textrm{ and } u(1,t) = 0 \\textrm{   for any    } t > 0$\n",
        "\n",
        "Initial conditions: $u(x,0) = u_0 = 0.5 \\textrm{   for any    } x \\in [-1,1]$\n",
        "    \n",
        "\n",
        "    \n",
        "In a previous notebook, you learned how to solve this problem using the SEM method. We will now solve this same example using the **Finite Difference Method**.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the Backward Finite Difference to approximate $\\frac{\\partial u}{\\partial t}$:\n",
        "    \n",
        "    \n",
        "$$ \\frac{\\partial u}{\\partial t} = \\frac{u(x_i, t^{(j)}) - u(x_i, t^{(j-1)}) }{\\Delta t} $$\n",
        "\n",
        "where we will consider equal time increments ${\\Delta t}  = t^{(j)} - t^{(j-1)}$.\n",
        "\n",
        "\n",
        "And we will use the Second order Central Finite Difference to approximate $\\frac{\\partial^2 u}{\\partial x^2} $:\n",
        "    \n",
        "    \n",
        "$$ \\frac{\\partial^2 u}{\\partial x^2}  = \\frac{u(x_{i+1}, t^{(j)}) - 2 \\, u(x_i, t^{(j)}) + u(x_{i-1}, t^{(j)}) }{\\Delta x} $$\n",
        "\n",
        "where we will consider equal spacing increments ${\\Delta x}  = x_{i+1} - x_i$.\n",
        "\n",
        "Plugging these approximations into the PDE, we get the following:\n",
        "\n",
        "\n",
        "$$ \\frac{D \\Delta t}{\\Delta x} \\left( - u(x_{i+1}, t^{(j)}) + 2 \\, u(x_i, t^{(j)}) - u(x_{i-1}, t^{(j)}) \\right) +  u(x_i, t^{(j)}) = u(x_i, t^{(j-1)}) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "which we can solve using the a time-stepping scheme as before: $\\mathbf{A}\\,\\mathbf{u}^{(j)} = \\mathbf{u}^{(j-1)}$ where \n",
        "\n",
        "$$ \\mathbf{u}^{(j)} = \\begin{bmatrix}  u(x_0, t^{(j)}) \\\\ u(x_1, t^{(j)}) \\\\ ... \\\\ u(x_{N-1}, t^{(j)}) \\\\ u(x_N, t^{(j)}) \\end{bmatrix} = \\begin{bmatrix}  0 \\\\ u(x_1, t^{(j)}) \\\\ ... \\\\ u(x_{N-1}, t^{(j)})  \\\\ 0 \\end{bmatrix} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we are only solving for the $N-1$ \"internal\" points, we need to construct a matrix with dimensions $N-1 \\times N-1$. We will use the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 50                      # number of spacing (intervals)\n",
        "x = np.linspace(-1,1,N+1)   # these are the discretized points (and so we have N + 1 points)\n",
        "dx = x[1] - x[0]\n",
        "\n",
        "D = 0.1              # thermal diffusivity\n",
        "dt = 0.05            # time increment\n",
        "timesteps = 400      # number of timesteps\n",
        "u0 = 0.5             # initial temperature in the rod\n",
        "\n",
        "utotal = np.zeros((x.shape[0],timesteps+1))  # to store all the temperatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write the initial vector  $\\mathbf{u}({\\bf x},t^0) = \\mathbf{u}_0$ (initial condition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "ut0 = u0*np.ones(N+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Update the first column of `utotal` using `ut0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "utotal[:,0] = ut0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the FD matrix ${\\bf A}$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "B = np.diag(2*np.ones(N-1)) - np.diag(np.ones(N-2),1) - np.diag(np.ones(N-2),-1)\n",
        "A = np.eye(N-1) + D*(dt/(dx**2))*B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use `plt.spy` to see structure of the matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.spy(A,markersize = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The matrix ${\\bf A}$ is called a `banded matrix`, because of the distribution of the non-zero entries. There are special solvers that take into consideration this type of storage, making the overall computational a lot more efficient (for example `scipy.linalg.solve_banded`). \n",
        "\n",
        "For the sake of simplicity, we will just use a \"general solve\" here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(timesteps):\n",
        "    u_old = utotal[:,i]\n",
        "    b = u_old[1:-1]  # remove the boundary conditions\n",
        "    u_new = sla.solve(A,b)\n",
        "    utotal[:,i+1] = np.append(0,np.append(u_new,0)) # add back the boundary condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\n",
        "ax.set_xlim((-1,1))\n",
        "ax.set_ylim((0,1.0))\n",
        "line, = ax.plot([],[],lw=2)\n",
        "\n",
        "def init():\n",
        "    line.set_data([],[])\n",
        "    return (line,)\n",
        "\n",
        "def animate(i):\n",
        "    y = utotal[:,i]\n",
        "    line.set_data(x,y)\n",
        "    return (line,)\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                               frames=100, interval=50, blit=True)\n",
        "\n",
        "rc('animation',html='html5')\n",
        "anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}